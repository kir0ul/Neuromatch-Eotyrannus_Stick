{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"GPU is not enabled in this notebook. \\n\"\n",
    "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook. \\n\"\n",
    "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is enabled in this notebook. \n",
      "If you want to disable it, in the menu under `Runtime` -> \n",
      "`Hardware accelerator.` and select `None` from the dropdown menu\n"
     ]
    }
   ],
   "source": [
    "device = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nma_drive = Path(\".\")\n",
    "abide_path = Path(nma_drive / \"ABIDE_pcp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_harmo = np.load(nma_drive / \"correlation_mat_harmo_whole.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the drive\n",
    "with open(nma_drive / \"abide_pcp_whole.pkl\", \"rb\") as f:\n",
    "    abide = pickle.load(f)\n",
    "# abide    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SUB_ID</th>\n",
       "      <th>X</th>\n",
       "      <th>subject</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>FILE_ID</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>DSM_IV_TR</th>\n",
       "      <th>AGE_AT_SCAN</th>\n",
       "      <th>...</th>\n",
       "      <th>qc_notes_rater_1</th>\n",
       "      <th>qc_anat_rater_2</th>\n",
       "      <th>qc_anat_notes_rater_2</th>\n",
       "      <th>qc_func_rater_2</th>\n",
       "      <th>qc_func_notes_rater_2</th>\n",
       "      <th>qc_anat_rater_3</th>\n",
       "      <th>qc_anat_notes_rater_3</th>\n",
       "      <th>qc_func_rater_3</th>\n",
       "      <th>qc_func_notes_rater_3</th>\n",
       "      <th>SUB_IN_SMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>2</td>\n",
       "      <td>50003</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24.45</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>3</td>\n",
       "      <td>50004</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>4</td>\n",
       "      <td>50005</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.73</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-parietal-cerebellum</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>5</td>\n",
       "      <td>50006</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.37</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-parietal slight</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>50007</td>\n",
       "      <td>6</td>\n",
       "      <td>50007</td>\n",
       "      <td>PITT</td>\n",
       "      <td>Pitt_0050007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.78</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-cerebellum_temporal_lob</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>1107</td>\n",
       "      <td>1108</td>\n",
       "      <td>51583</td>\n",
       "      <td>1108</td>\n",
       "      <td>51583</td>\n",
       "      <td>SBL</td>\n",
       "      <td>SBL_0051583</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>35.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>ic-cerebellum-temporal_lobe</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>1108</td>\n",
       "      <td>1109</td>\n",
       "      <td>51584</td>\n",
       "      <td>1109</td>\n",
       "      <td>51584</td>\n",
       "      <td>SBL</td>\n",
       "      <td>SBL_0051584</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>49.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>vmpfc dropout</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>1109</td>\n",
       "      <td>1110</td>\n",
       "      <td>51585</td>\n",
       "      <td>1110</td>\n",
       "      <td>51585</td>\n",
       "      <td>SBL</td>\n",
       "      <td>SBL_0051585</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-cerebellum-temporal_lobe</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>1110</td>\n",
       "      <td>1111</td>\n",
       "      <td>51606</td>\n",
       "      <td>1111</td>\n",
       "      <td>51606</td>\n",
       "      <td>MAX_MUN</td>\n",
       "      <td>MaxMun_a_0051606</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-cerebellum</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>1111</td>\n",
       "      <td>1112</td>\n",
       "      <td>51607</td>\n",
       "      <td>1112</td>\n",
       "      <td>51607</td>\n",
       "      <td>MAX_MUN</td>\n",
       "      <td>MaxMun_a_0051607</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26.00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maybe</td>\n",
       "      <td>ic-cerebellum</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         i  Unnamed: 0  SUB_ID     X  subject  SITE_ID           FILE_ID  \\\n",
       "1        1           2   50003     2    50003     PITT      Pitt_0050003   \n",
       "2        2           3   50004     3    50004     PITT      Pitt_0050004   \n",
       "3        3           4   50005     4    50005     PITT      Pitt_0050005   \n",
       "4        4           5   50006     5    50006     PITT      Pitt_0050006   \n",
       "5        5           6   50007     6    50007     PITT      Pitt_0050007   \n",
       "...    ...         ...     ...   ...      ...      ...               ...   \n",
       "1107  1107        1108   51583  1108    51583      SBL       SBL_0051583   \n",
       "1108  1108        1109   51584  1109    51584      SBL       SBL_0051584   \n",
       "1109  1109        1110   51585  1110    51585      SBL       SBL_0051585   \n",
       "1110  1110        1111   51606  1111    51606  MAX_MUN  MaxMun_a_0051606   \n",
       "1111  1111        1112   51607  1112    51607  MAX_MUN  MaxMun_a_0051607   \n",
       "\n",
       "      DX_GROUP  DSM_IV_TR  AGE_AT_SCAN  ...  qc_notes_rater_1 qc_anat_rater_2  \\\n",
       "1            1          1        24.45  ...               NaN              OK   \n",
       "2            1          1        19.09  ...               NaN              OK   \n",
       "3            1          1        13.73  ...               NaN              OK   \n",
       "4            1          1        13.37  ...               NaN              OK   \n",
       "5            1          1        17.78  ...               NaN              OK   \n",
       "...        ...        ...          ...  ...               ...             ...   \n",
       "1107         1          2        35.00  ...               NaN              OK   \n",
       "1108         1          2        49.00  ...               NaN              OK   \n",
       "1109         1          1        27.00  ...               NaN              OK   \n",
       "1110         1          2        29.00  ...               NaN              OK   \n",
       "1111         1          2        26.00  ...               NaN              OK   \n",
       "\n",
       "      qc_anat_notes_rater_2  qc_func_rater_2        qc_func_notes_rater_2  \\\n",
       "1                       NaN               OK                          NaN   \n",
       "2                       NaN               OK                          NaN   \n",
       "3                       NaN            maybe       ic-parietal-cerebellum   \n",
       "4                       NaN            maybe           ic-parietal slight   \n",
       "5                       NaN            maybe   ic-cerebellum_temporal_lob   \n",
       "...                     ...              ...                          ...   \n",
       "1107                    NaN               OK  ic-cerebellum-temporal_lobe   \n",
       "1108                    NaN            maybe                vmpfc dropout   \n",
       "1109                    NaN            maybe  ic-cerebellum-temporal_lobe   \n",
       "1110                    NaN            maybe                ic-cerebellum   \n",
       "1111                    NaN            maybe                ic-cerebellum   \n",
       "\n",
       "      qc_anat_rater_3 qc_anat_notes_rater_3 qc_func_rater_3  \\\n",
       "1                  OK                   NaN              OK   \n",
       "2                  OK                   NaN              OK   \n",
       "3                  OK                   NaN              OK   \n",
       "4                  OK                   NaN              OK   \n",
       "5                  OK                   NaN              OK   \n",
       "...               ...                   ...             ...   \n",
       "1107               OK                   NaN              OK   \n",
       "1108               OK                   NaN              OK   \n",
       "1109               OK                   NaN              OK   \n",
       "1110               OK                   NaN              OK   \n",
       "1111               OK                   NaN              OK   \n",
       "\n",
       "     qc_func_notes_rater_3  SUB_IN_SMP  \n",
       "1                      NaN           1  \n",
       "2                      NaN           1  \n",
       "3                      NaN           0  \n",
       "4                      NaN           1  \n",
       "5                      NaN           1  \n",
       "...                    ...         ...  \n",
       "1107                   NaN           0  \n",
       "1108                   NaN           0  \n",
       "1109                   NaN           0  \n",
       "1110                   NaN           0  \n",
       "1111                   NaN           1  \n",
       "\n",
       "[871 rows x 106 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phen_abide = abide['phenotypic']\n",
    "phen_abide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_len(corr_mat): \n",
    "\n",
    "    a_dims = np.array([item.shape for item in corr_mat])\n",
    "\n",
    "    rois_dim = np.max(a_dims[:, 1])\n",
    "    ts_dim = np.max(a_dims[:, 0])\n",
    "\n",
    "    x_data = np.zeros((ts_dim, rois_dim, len(corr_mat)))\n",
    "\n",
    "    for idx, val in enumerate(corr_mat):\n",
    "        x_data[0:val.shape[0], 0:val.shape[1], idx] = val\n",
    "\n",
    "    return x_data\n",
    "\n",
    "def gender_selection(phen_abide, corr_harmo, y_target): \n",
    "\n",
    "    '''\n",
    "    :param phen_abide: phenotypic Pandas Dataframe extracted at the beggining of the notebook\n",
    "\n",
    "    :param corr_harmo: correlation matrix array with the whole dataset - harmonized\n",
    "\n",
    "    :param y_target: y_target array with vales [1,0]\n",
    "\n",
    "    :return: X_data, y_target for male and female respectively\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # new_indices = np.arange(0, 172, 1)\n",
    "    new_indices = np.arange(0, len(phen_abide), 1)\n",
    "\n",
    "    idx_phen_abide = phen_abide.set_index(new_indices)\n",
    "\n",
    "    males_stat = idx_phen_abide[idx_phen_abide['SEX'] == 1]\n",
    "    males_index = np.array(males_stat.index)\n",
    "\n",
    "    females_stat = idx_phen_abide[idx_phen_abide['SEX'] == 2]\n",
    "    females_index = np.array(females_stat.index)\n",
    "\n",
    "    males_data = corr_harmo[males_index]\n",
    "    females_data = corr_harmo[females_index]\n",
    "\n",
    "    print(f'Male harmonized matrix: {males_data.shape}, Female harmonized matrix: {females_data.shape}')\n",
    "\n",
    "    #rois_dim = np.max(a_dims[:, 1])\n",
    "    dx_data_males = y_target[males_index]\n",
    "    dx_data_females = y_target[females_index]\n",
    "\n",
    "    # Equal series length\n",
    "\n",
    "    x_data_males = equal_len(males_data)\n",
    "    x_data_females = equal_len(females_data)\n",
    "\n",
    "    print(f'Final X_data shape from males: {x_data_males.shape}, Final X_data shape from females: {x_data_females.shape}')\n",
    "\n",
    "    return x_data_males, dx_data_males, x_data_females, dx_data_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((871, 2), 200, 316)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dims = np.array([item.shape for item in abide['rois_cc200']])\n",
    "rois_dim = np.max(a_dims[:, 1])\n",
    "ts_dim = np.max(a_dims[:, 0])\n",
    "a_dims.shape, rois_dim, ts_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316, 200, 871)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.zeros((ts_dim, rois_dim, len(abide['rois_cc200'])))\n",
    "for idx, val in enumerate(abide['rois_cc200']):\n",
    "    x_data[0:val.shape[0], 0:val.shape[1], idx] = val\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 1 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 1 2\n",
      " 2 1 1 2 2 1 1 1 1 1 2 2 1 2 1 2 1 1 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1\n",
      " 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n",
      " 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n",
      " 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 871 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "dx_counts = phen_abide['DX_GROUP']\n",
    "y_target = np.array(dx_counts)\n",
    "print(y_target, len(y_target), type(y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(input_list):\n",
    "    output_list = [0 if x == 1 else 1 if x == 2 else x for x in input_list]\n",
    "    return np.array(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 871\n"
     ]
    }
   ],
   "source": [
    "y_target = replace_values(y_target)\n",
    "print(y_target, len(y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male harmonized matrix: (727, 200, 200), Female harmonized matrix: (144, 200, 200)\n",
      "Final X_data shape from males: (200, 200, 727), Final X_data shape from females: (200, 200, 144)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((200, 200, 727), (727,), (200, 200, 144), (144,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_males, dx_data_males, x_data_females, dx_data_females = gender_selection(phen_abide, corr_harmo, y_target)\n",
    "x_data_males.shape, dx_data_males.shape, x_data_females.shape, dx_data_females.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split_data(X, y, seed):\n",
    "  \"\"\"\n",
    "  Helper function to shuffle and split incoming data\n",
    "\n",
    "  Args:\n",
    "    X: torch.tensor\n",
    "      Input data\n",
    "    y: torch.tensor\n",
    "      Corresponding target variables\n",
    "    seed: int\n",
    "      Set seed for reproducibility\n",
    "\n",
    "  Returns:\n",
    "    X_test: torch.tensor\n",
    "      Test data [20% of X]\n",
    "    y_test: torch.tensor\n",
    "      Labels corresponding to above mentioned test data\n",
    "    X_train: torch.tensor\n",
    "      Train data [80% of X]\n",
    "    y_train: torch.tensor\n",
    "      Labels corresponding to above mentioned train data\n",
    "  \"\"\"\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  X = np.asarray(X)\n",
    "  # Number of samples\n",
    "  N = X.shape[2]\n",
    "  # print(N)\n",
    "    \n",
    "  # Shuffle data\n",
    "  shuffled_indices = torch.randperm(N)  # Get indices to shuffle data, could use torch.randperm\n",
    "  X = X[shuffled_indices]\n",
    "  y = y[shuffled_indices]\n",
    "\n",
    "  # Split data into train/test\n",
    "  test_size = int(0.2 * N)    # Assign test datset size using 20% of samples\n",
    "  X_test = X[:test_size]\n",
    "  y_test = y[:test_size]\n",
    "  X_train = X[test_size:]\n",
    "  y_train = y[test_size:]\n",
    "\n",
    "  return torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float(), torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 519 is out of bounds for axis 0 with size 200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_test, y_test, X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mshuffle_and_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data_males\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx_data_males\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m X_test\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape, X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[22], line 32\u001b[0m, in \u001b[0;36mshuffle_and_split_data\u001b[0;34m(X, y, seed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(N)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m   \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Shuffle data\u001b[39;00m\n\u001b[1;32m     31\u001b[0m shuffled_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(N)  \u001b[38;5;66;03m# Get indices to shuffle data, could use torch.randperm\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshuffled_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m y \u001b[38;5;241m=\u001b[39m y[shuffled_indices]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Split data into train/test\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 519 is out of bounds for axis 0 with size 200"
     ]
    }
   ],
   "source": [
    "X_test, y_test, X_train, y_train = shuffle_and_split_data(x_data_males, dx_data_males, seed=SEED)\n",
    "X_test.shape, y_test.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_train_test_reshape(X_train, X_test):\n",
    "    # Convertir X_train a una matriz de dos dimensiones (8, 200*200)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    print(X_train.shape)\n",
    "    # Convertir X_test a una matriz de dos dimensiones (2, 200*200)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    print(X_test.shape)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train_test_reshape(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "batch_size = 32\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=0,\n",
    "                         worker_init_fn=seed_worker,\n",
    "                         generator=g_seed)\n",
    "\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True,\n",
    "                          shuffle=True, num_workers=0,\n",
    "                          worker_init_fn=seed_worker,\n",
    "                          generator=g_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape, y_test.shape, X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  \"\"\"\n",
    "  Initialize MLP Network\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, actv, input_feature_num, hidden_unit_nums, output_feature_num):\n",
    "    \"\"\"\n",
    "    Initialize MLP Network parameters\n",
    "\n",
    "    Args:\n",
    "      actv: string\n",
    "        Activation function\n",
    "      input_feature_num: int\n",
    "        Number of input features\n",
    "      hidden_unit_nums: list\n",
    "        Number of units in the hidden layer\n",
    "      output_feature_num: int\n",
    "        Number of output features\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(Net, self).__init__()\n",
    "    self.input_feature_num = input_feature_num # Save the input size for reshaping later\n",
    "    self.mlp = nn.Sequential() # Initialize layers of MLP\n",
    "\n",
    "    in_num = input_feature_num # Initialize the temporary input feature to each layer\n",
    "    for i in range(len(hidden_unit_nums)): # Loop over layers and create each one\n",
    "\n",
    "      out_num = hidden_unit_nums[i] # Assign the current layer hidden unit from list\n",
    "      layer = nn.Linear(in_num, out_num) # Use nn.Linear to define the layer\n",
    "      in_num = out_num # Assign next layer input using current layer output\n",
    "      self.mlp.add_module('Linear_%d'%i, layer) # Append layer to the model with a name\n",
    "\n",
    "      actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n",
    "      self.mlp.add_module('Activation_%d'%i, actv_layer) # Append activation to the model with a name\n",
    "\n",
    "    out_layer = nn.Linear(in_num, output_feature_num) # Create final layer\n",
    "    self.mlp.add_module('Output_Linear', out_layer) # Append the final layer\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Simulate forward pass of MLP Network\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input data\n",
    "\n",
    "    Returns:\n",
    "      logits: Instance of MLP\n",
    "        Forward pass of MLP\n",
    "    \"\"\"\n",
    "    # Reshape inputs to (batch_size, input_feature_num)\n",
    "    # Just in case the input vector is not 2D, like an image!\n",
    "    x = x.view(-1, self.input_feature_num)\n",
    "\n",
    "    logits = self.mlp(x) # Forward pass of MLP\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_classification(net, criterion, optimizer, train_loader,\n",
    "                              test_loader, num_epochs=1, verbose=True,\n",
    "                              training_plot=False, device='cpu'):\n",
    "  \"\"\"\n",
    "  Accumulate training loss/Evaluate performance\n",
    "\n",
    "  Args:\n",
    "    net: instance of Net class\n",
    "      Describes the model with ReLU activation, batch size 128\n",
    "    criterion: torch.nn type\n",
    "      Criterion combines LogSoftmax and NLLLoss in one single class.\n",
    "    optimizer: torch.optim type\n",
    "      Implements Adam algorithm.\n",
    "    train_loader: torch.utils.data type\n",
    "      Combines the train dataset and sampler, and provides an iterable over the given dataset.\n",
    "    test_loader: torch.utils.data type\n",
    "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
    "    num_epochs: int\n",
    "      Number of epochs [default: 1]\n",
    "    verbose: boolean\n",
    "      If True, print statistics\n",
    "    training_plot=False\n",
    "      If True, display training plot\n",
    "    device: string\n",
    "      CUDA/GPU if available, CPU otherwise\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  net.train()\n",
    "  training_losses = []\n",
    "  for epoch in tqdm(range(num_epochs)):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "      # Get the inputs; data is a list of [inputs, labels]\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device).float()\n",
    "      labels = labels.to(device).long()\n",
    "\n",
    "      # Zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = net(inputs)\n",
    "\n",
    "      # print(outputs, type(outputs), outputs.shape)\n",
    "      # print(labels.shape, type(labels), labels)\n",
    "\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Print statistics\n",
    "      if verbose:\n",
    "        training_losses += [loss.item()]\n",
    "\n",
    "  net.eval()\n",
    "\n",
    "  def test(data_loader):\n",
    "    \"\"\"\n",
    "    Function to gauge network performance\n",
    "\n",
    "    Args:\n",
    "      data_loader: torch.utils.data type\n",
    "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
    "\n",
    "    Returns:\n",
    "      acc: float\n",
    "        Performance of the network\n",
    "      total: int\n",
    "        Number of datapoints in the dataloader\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    precision = []\n",
    "    recall = []\n",
    "    F1 = []\n",
    "    balanced_acc = []\n",
    "    for data in data_loader:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device).float()\n",
    "      labels = labels.cpu().long()\n",
    "\n",
    "      outputs = net(inputs)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      predicted = predicted.cpu()\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "      precision.append(metrics.precision_score(labels,predicted))\n",
    "      recall.append(metrics.recall_score(labels,predicted))\n",
    "      F1.append(metrics.f1_score(labels,predicted))\n",
    "      balanced_acc.append(metrics.balanced_accuracy_score(labels, predicted))\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total, acc, precision, recall, F1, balanced_acc, predicted, labels\n",
    "\n",
    "  train_total, train_acc, precision_train, recall_train, F1_train,balanced_acc, y_pred_train, y_true_train = test(train_loader)\n",
    "  test_total, test_acc, precision_test, recall_test, F1_test, balanced_acc_test, y_pred_test, y_true_test = test(test_loader)\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"Accuracy on the {train_total} training samples: {train_acc:0.2f}\")\n",
    "    print(f\"Accuracy on the {test_total} testing samples: {test_acc:0.2f}\")\n",
    "    \n",
    "    # More metrics\n",
    "    print(f\"Balanced accuracy on training {balanced_acc}, on testing samples: {balanced_acc_test}\"),\n",
    "    print(f\"Precision training {precision_train} testing samples: {precision_test}\"),\n",
    "    print(f\"Recall training {recall_train} Recall testing samples: {recall_test}\"),\n",
    "    print(f\"F1 training {F1_train} F1 testing samples: {F1_test}\"),\n",
    "    # print('TRAINING Y: ', y_pred_train, y_true_train),\n",
    "    # print('TESTING Y: ', y_pred_test, y_true_test),\n",
    "    cnf_mat_train = metrics.confusion_matrix(y_pred_train, y_true_train)\n",
    "    cnf_mat_test = metrics.confusion_matrix(y_pred_test, y_true_test)\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    df_cm = pd.DataFrame(cnf_mat_train, range(2), range(2))\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=\"crest\") # font size\n",
    "    plt.title('Confusion Matrix Train',fontsize=16)\n",
    "    plt.show()\n",
    "    fig2,ax2 = plt.subplots()\n",
    "    df_cm = pd.DataFrame(cnf_mat_test, range(2), range(2))\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, cmap=\"crest\") # font size\n",
    "    plt.title('Confusion Matrix Test',fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "  if training_plot:\n",
    "    plt.plot(training_losses)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.title('Loss Function')\n",
    "    plt.show()\n",
    "\n",
    "  return train_acc, test_acc, training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "net = Net('ReLU()', X_train.shape[1], [32], 2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "num_epochs = 150\n",
    "\n",
    "_, _, training_losses = train_test_classification(net, criterion, optimizer, train_loader,\n",
    "                                 test_loader, num_epochs=num_epochs,\n",
    "                                 training_plot=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
